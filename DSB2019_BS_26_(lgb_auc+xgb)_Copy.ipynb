{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSB2019 - BS 26 (lgb/auc+xgb) Copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1QnY6R+mcTVmOKAPsAMy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/kaggle/blob/master/DSB2019_BS_26_(lgb_auc%2Bxgb)_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvQ5yqHWf9fN",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "https://www.kaggle.com/steubk/dsb2019-bs-26-lgb-auc-xgb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56f4otL4NpCk",
        "colab_type": "code",
        "outputId": "da3f7e3e-b173-4f37-ec32-6502d7eeff59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename =  \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXAgIGRHN85t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "%cd /content/LightGBM/\n",
        "!mkdir build\n",
        "!cmake -DUSE_GPU=1 #avoid ..\n",
        "!make -j$(nproc)\n",
        "!sudo apt-get -y install python-pip\n",
        "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        "%cd /content/LightGBM/python-package\n",
        "!sudo python setup.py install --precompile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-TG-Xe8OJ6F",
        "colab_type": "code",
        "outputId": "37229104-ff2b-4f41-924c-d2ecd024a57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HITmv9PkOLar",
        "colab_type": "code",
        "outputId": "8d896853-ad46-463c-99d1-bd78ec65ebae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!kaggle competitions list"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                            deadline             category             reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ----------  ---------  --------------  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started   Knowledge       2363           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started   Knowledge      15980            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started   Knowledge       5067            True  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started   Knowledge        373           False  \n",
            "imagenet-object-localization-challenge         2029-12-31 07:00:00  Research          Knowledge         63           False  \n",
            "competitive-data-science-predict-future-sales  2020-12-31 23:59:00  Playground            Kudos       5602            True  \n",
            "deepfake-detection-challenge                   2020-03-31 23:59:00  Featured         $1,000,000       1414           False  \n",
            "cat-in-the-dat-ii                              2020-03-31 23:59:00  Playground             Swag        431           False  \n",
            "nlp-getting-started                            2020-03-23 23:59:00  Getting Started     $10,000       2257           False  \n",
            "bengaliai-cv19                                 2020-03-16 23:59:00  Research            $10,000        952           False  \n",
            "google-quest-challenge                         2020-02-10 23:59:00  Featured            $25,000       1388           False  \n",
            "tensorflow2-question-answering                 2020-01-22 23:59:00  Featured            $50,000       1233           False  \n",
            "data-science-bowl-2019                         2020-01-22 23:59:00  Featured           $160,000       3497            True  \n",
            "pku-autonomous-driving                         2020-01-21 23:59:00  Featured            $25,000        866           False  \n",
            "santa-2019-revenge-of-the-accountants          2020-01-16 23:59:00  Playground             Swag        106           False  \n",
            "santa-workshop-tour-2019                       2020-01-15 23:59:00  Featured            $25,000       1620            True  \n",
            "nfl-big-data-bowl-2020                         2020-01-06 23:59:00  Featured            $75,000       2038           False  \n",
            "nfl-playing-surface-analytics                  2020-01-02 23:59:00  Analytics           $75,000          0           False  \n",
            "ashrae-energy-prediction                       2019-12-19 23:59:00  Featured            $25,000       3614           False  \n",
            "Kannada-MNIST                                  2019-12-17 23:59:00  Playground        Knowledge       1214           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ6NerPEOMpU",
        "colab_type": "code",
        "outputId": "7c8a7c46-3025-422e-cc5e-a064348a1e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!kaggle competitions download -c data-science-bowl-2019"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_labels.csv.zip to /content\n",
            "  0% 0.00/262k [00:00<?, ?B/s]\n",
            "100% 262k/262k [00:00<00:00, 81.9MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/10.8k [00:00<?, ?B/s]\n",
            "100% 10.8k/10.8k [00:00<00:00, 11.3MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 96% 382M/397M [00:06<00:00, 64.2MB/s]\n",
            "100% 397M/397M [00:06<00:00, 66.2MB/s]\n",
            "Downloading specs.csv to /content\n",
            "  0% 0.00/399k [00:00<?, ?B/s]\n",
            "100% 399k/399k [00:00<00:00, 126MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 81% 33.0M/40.8M [00:01<00:00, 8.29MB/s]\n",
            "100% 40.8M/40.8M [00:01<00:00, 33.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKT4dwqNPs4z",
        "colab_type": "code",
        "outputId": "8913aa60-47bc-4562-9d84-81e436195ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# !pip install bayesian-optimization"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=692131f93c316c0d65652883a8d4f9a939d480b462a5fab49a10e2cab6ed6797\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gX49NTPPcf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from random import seed\n",
        "from random import randint\n",
        "# seed random number generator\n",
        "seed(44)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from numba import jit \n",
        "from scipy import stats\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# garbage collection\n",
        "import gc\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "  return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def eval_qwk_lgb_regr(y_pred, accuracy_groups):\n",
        "  \"\"\"\n",
        "  Fast kappa eval function for lgb.\n",
        "  \"\"\"\n",
        "  # 辞書形式で集計してくれる\n",
        "  dist = Counter(accuracy_groups)\n",
        "  # それらを比率のデータに変える\n",
        "  for k in dist:\n",
        "    dist[k] /= len(accuracy_groups)\n",
        "  \n",
        "  acum = 0\n",
        "  bound = {}\n",
        "\n",
        "  # 予測した値を累積の割合に応じてパーセンタイルで表現する。\n",
        "  for i in range(3):\n",
        "    acum += dist[i]\n",
        "    bound[i] = np.percentile(y_pred, acum*100)\n",
        "\n",
        "  def classify(x):\n",
        "    if x <= bound[0]:\n",
        "      return 0\n",
        "    elif x <= bound[1]:\n",
        "      return 1\n",
        "    elif x <= bound[2]:\n",
        "      return 2\n",
        "    else:\n",
        "      return 3\n",
        "    \n",
        "  y_pred = np.array(list(map(classify, y_pred)))\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "@jit\n",
        "# quadratic weighted kappa\n",
        "def qwk3(a1, a2, max_rat=3):\n",
        "  # 条件式がTrueではない時に、例外を投げてくれる関数\n",
        "  # 同じでない場合に、AssertionErrorが発生する\n",
        "  assert(len(a1) == len(a2))\n",
        "  a1 = np.asarray(a1, dtype=int)\n",
        "  a2 = np.asarray(a2, dtype=int)\n",
        "\n",
        "  hist1 = np.zeros((max_rat + 1, ))\n",
        "  hist2 = np.zeros((max_rat + 1, ))\n",
        "\n",
        "  o = 0\n",
        "  for k in range(a1.shape[0]):\n",
        "    i, j = a1[k], a2[k]\n",
        "    hist1[i] += 1\n",
        "    hist2[j] += 1\n",
        "    o += (i - j) * (i - j)\n",
        "\n",
        "  e = 0\n",
        "\n",
        "  for i in range(max_rat + 1):\n",
        "    for j in range(max_rat + 1):\n",
        "      e += hist1[i] * hist2[j] * (i - j)*(i - j)\n",
        "\n",
        "  e = e / a1.shape[0]\n",
        "\n",
        "  return 1 - o / e\n",
        "\n",
        "def round_prediction(x, a, b, c):\n",
        "  x = np.where(x < a, 0, x)\n",
        "  x = np.where((a < x) & (x <= a + b), 1, x)\n",
        "  x = np.where((a + b < x) & (x <= a + b + c), 2, x)\n",
        "  x = np.where((a + b + c < x), 3, x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttTd2UA5U-Tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3165387e-c557-42d1-e85f-0d76396b2df4"
      },
      "source": [
        "ls"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json      sample_submission.csv  test.csv.zip   train_labels.csv.zip\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  specs.csv              train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsWS2liBOoJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataframe_from64_to32_converter(dataframe):\n",
        "  import numpy as np\n",
        "  for column in dataframe.columns:\n",
        "    if dataframe[column].dtype == 'float64':\n",
        "      dataframe[column] = dataframe[column].astype(np.float32)\n",
        "    if dataframe[column].dtype == 'int64':\n",
        "      dataframe[column] = dataframe[column].astype(np.int32)\n",
        "  return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trrpfcAeOOH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "  print('Reading train.csv file....')\n",
        "  train = pd.read_csv('/content/train.csv.zip')\n",
        "  train = dataframe_from64_to32_converter(train)\n",
        "  print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
        "\n",
        "  print('Reading test.csv file....')\n",
        "  test = pd.read_csv('/content/test.csv.zip')\n",
        "  test = dataframe_from64_to32_converter(test)\n",
        "  print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
        "\n",
        "  print('Reading train_labels.csv file....')\n",
        "\n",
        "  train_labels = pd.read_csv('/content/train_labels.csv')\n",
        "  print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
        "\n",
        "  print('Reading specs.csv file....')\n",
        "  specs = pd.read_csv('/content/specs.csv')\n",
        "  print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
        "\n",
        "  print('Reading sample_submission.csv file....')\n",
        "\n",
        "  sample_submission = pd.read_csv('/content/sample_submission.csv')\n",
        "  print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
        "  return train, test, train_labels, specs, sample_submission\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjsyiWgaWFSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_title(train, test, train_labels):\n",
        "  print(\"Start encoding data\")\n",
        "\n",
        "  # encode title\n",
        "  train['title_event_code'] = sorted(list(map(lambda x, y :str(x) + '_' + str(y), train['title'], train['event_code'])))\n",
        "  test['title_event_code'] = sorted(list(map(lambda x, y :str(x) + '_' + str(y), test['title'], test['event_code'])))\n",
        "  all_title_event_code = sorted(list(set(train['title_event_code'].unique()).union(test['title_event_code'].unique())))\n",
        "\n",
        "  # world\n",
        "  train['type_world'] = sorted(list(map(lambda x, y : str(x) + '_' + str(y), train['type'], train['world'])))\n",
        "  test['type_world'] = sorted(list(map(lambda x, y : str(x) + '_' + str(y), test['type'], test['world'])))\n",
        "  all_type_world = sorted(list(set(train['type_world'].unique()).union(test['type_world'].unique())))\n",
        "\n",
        "  # make a list with all the unique 'titles' from the train and test set\n",
        "  list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
        "\n",
        "  # make a list with all the unique 'event_code' from the train and test set\n",
        "  list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
        "  list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n",
        "\n",
        "  # make a list with all the unique worlds from the train and test set\n",
        "  list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
        "\n",
        "  # create a dictionary numerating the titles\n",
        "  activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
        "  activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
        "\n",
        "  activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
        "  assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).\\\n",
        "                              union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n",
        "  \n",
        "  # replace the text titles with the number titles from the dict\n",
        "  train['title'] = train['title'].map(activities_map)\n",
        "  test['title']  = test['title'].map(activities_map)\n",
        "  train['world'] = train['world'].map(activities_world)\n",
        "  test['world'] = test['world'].map(activities_world)\n",
        "  train_labels['title'] = train_labels['title'].map(activities_map)\n",
        "  win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
        "\n",
        "  # then, it set one element, the 'Bird Measurer(Assessment)' as 4110, 10 more than the rest\n",
        "  win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
        "\n",
        "  # convert text into datetime\n",
        "  train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
        "  test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
        "\n",
        "  event_data = {}\n",
        "  event_data['train_labels'] = train_labels\n",
        "  event_data['win_code'] = win_code\n",
        "  event_data['list_of_user_activities'] = list_of_user_activities\n",
        "  event_data['list_of_event_code'] = list_of_event_code\n",
        "  event_data[\"activities_labels\"] = activities_labels\n",
        "  event_data[\"assess_titles\"] = assess_titles\n",
        "  event_data[\"list_of_event_id\"] = list_of_event_id\n",
        "  event_data[\"all_title_event_code\"] = all_title_event_code\n",
        "  event_data[\"activities_map\"] = activities_map\n",
        "  event_data[\"all_type_world\"] = all_type_world\n",
        "\n",
        "  return train, test, event_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXoaMms0kKTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_features(feature_dict, ac_data):\n",
        "  if len(ac_data['durations']) > 0:\n",
        "    feature_dict['installation_duration_mean'] = np.mean(ac_data['durations'])\n",
        "    feature_dict['installation_duration_sum'] = np.sum(ac_data['durations'])\n",
        "\n",
        "  else:\n",
        "    feature_dict['installation_duration_mean'] = 0\n",
        "    feature_dict['installation_duration_sum'] = 0\n",
        "\n",
        "  return feature_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rjF_l5XmFf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(user_sample, event_data, test_set):\n",
        "  '''\n",
        "    The user_sample is a DataFrame from train or test where the only one\n",
        "    installation_id is filtered\n",
        "    And the test_set parameter is related with the labels processing, that is only requered\n",
        "    if test_set=False\n",
        "  '''\n",
        "  # Constants and parameters declaration\n",
        "  last_assessment = {}\n",
        "\n",
        "  last_activity = 0\n",
        "  user_activities_count = {'Clip':0,\n",
        "                           'Activity':0,\n",
        "                           'Assessment':0,\n",
        "                           'Game':0}\n",
        "  assess_4020_acc_dict = {'Clip_gametime': 0, 'Game_gametime': 0,\n",
        "                             'Activity_gametime': 0, 'Assessment_gametime': 0}\n",
        "\n",
        "  last_session_time_sec = 0\n",
        "  accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "  all_assessments = []\n",
        "  accumulated_accuracy_group = 0\n",
        "  accumulated_accuracy = 0\n",
        "  accumulated_correct_attempts = 0\n",
        "  accumulated_uncorrect_attempts = 0\n",
        "  accumulated_actions = 0\n",
        "\n",
        "  accumulated_game_miss = 0\n",
        "  Cauldron_Filler_4025 = 0\n",
        "  mean_game_round = 0\n",
        "  mean_game_duration = 0\n",
        "  mean_game_level = 0\n",
        "  Assessment_mean_event_count = 0\n",
        "  Game_mean_event_count = 0\n",
        "  Activity_mean_event_count = 0\n",
        "  chest_assessment_uncorrect_sum = 0\n",
        "\n",
        "  counter = 0\n",
        "  time_first_activity = float(user_sample['timestamp'].values[0])\n",
        "  durations = []\n",
        "  durations_game = []\n",
        "  durations_activity = []\n",
        "  last_accuracy_title = {'acc_' + title: -1 for title in event_data[\"assess_titles\"]}\n",
        "  last_game_time_title = {'lgt_' + title: 0 for title in event_data[\"assess_titles\"]}\n",
        "  ac_game_time_title = {'agt_' + title: 0 for title in event_data[\"assess_titles\"]}\n",
        "  ac_true_attempts_title = {'ata_' + title: 0 for title in event_data[\"assess_titles\"]}\n",
        "  ac_false_attempts_title = {'afa_' + title: 0 for title in event_data[\"assess_titles\"]}\n",
        "  event_code_count: dict[str, int] = {ev: 0 for ev in event_data[\"list_of_event_code\"]}\n",
        "  event_code_proc_count = {str(ev) + \"_proc\" : 0. for ev in event_data[\"list_of_event_code\"]}\n",
        "  event_id_count: dict[str, int] = {eve: 0 for eve in event_data[\"list_of_event_id\"]}\n",
        "  title_count: dict[str, int] = {eve: 0 for eve in event_data[\"activities_labels\"].values()}\n",
        "  title_event_code_count: dict[str, int] = {t_eve: 0 for t_eve in event_data[\"all_title_event_code\"]}\n",
        "  type_world_count: dict[str, int] = {w_eve: 0 for w_eve in event_data[\"all_type_world\"]}\n",
        "  session_count = 0\n",
        "\n",
        "  Activity_game_durations = [] \n",
        "\n",
        "  last_Game_Features = {}\n",
        "  last_game_session_correct_true = np.nan\n",
        "  last_game_session_correct_false = np.nan\n",
        "\n",
        "  last_2_game_session_correct_true = np.nan\n",
        "  last_2_game_session_correct_false = np.nan\n",
        "\n",
        "  acc_game_session_correct_true = 0\n",
        "  acc_game_session_correct_false = 0\n",
        "\n",
        "  last_Assessment_Features = {}\n",
        "\n",
        "  last_Activity_Features = {}\n",
        "  session_type_story = []\n",
        "\n",
        "  # iterates through each session of one installation_id\n",
        "  for i, session in user_sample.groupby('game_session', sort=False):\n",
        "    # set some sessions information\n",
        "    session_type = session['type'].iloc[0]\n",
        "    session_title = session['title'].iloc[0]\n",
        "    session_title_text = event_data[\"activities_labels\"][session_title]\n",
        "    game_session = session['game_session'].iloc[0]    \n",
        "    session_world = session['world'].iloc[0]\n",
        "    timestamp = session['timestamp'].iloc[0]\n",
        "\n",
        "    if session_type == \"Activity\":\n",
        "      Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n",
        "      durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
        "\n",
        "      last_Activity_Features = {\n",
        "                \"last_activity_event_count\":session[\"event_count\"].values[-1],\n",
        "                \"last_activity_event_count_nunique\" : session[\"event_count\"].nunique(),\n",
        "                \"last_activity_timestamp\": timestamp,\n",
        "                \"last_activity_world\": session_world, \n",
        "                \"last_activity_session_title\": session_title\n",
        "            }\n",
        "      \n",
        "    if session_type == \"Game\":\n",
        "      Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n",
        "\n",
        "      game_s = session[session.event_code == 2030]\n",
        "      misses_cnt = cnt_miss(game_s)\n",
        "      accumulated_game_miss += misses_cnt\n",
        "\n",
        "      # json形式のデータの抽出\n",
        "      try:\n",
        "        game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n",
        "        mean_game_round = (mean_game_round + game_round) / 2.0\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "          game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n",
        "          mean_game_duration = (mean_game_duration + game_duration) / 2.0\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      try:\n",
        "          game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n",
        "          mean_game_level = (mean_game_level + game_level) / 2.0\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      game_session_correct = session[session['event_data'].map(lambda x: '\"correct\"' in x)]['event_data']\\\n",
        "                                        .map(lambda x: json.loads(x)['correct'])\n",
        "      last_2_game_session_correct_true = last_game_session_correct_true\n",
        "      last_2_game_session_correct_false = last_game_session_correct_false\n",
        "\n",
        "      last_game_session_correct_true = game_session_correct.sum()\n",
        "      last_game_session_correct_false = game_session_correct.shape[0] - last_game_session_correct_true\n",
        "      acc_game_session_correct_true += last_game_session_correct_true\n",
        "      acc_game_session_correct_false += last_game_session_correct_false\n",
        "\n",
        "      durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
        "\n",
        "      last_Game_Features = {\n",
        "                \"last_game_event_count\":session[\"event_count\"].values[-1],\n",
        "                \"last_game_event_count_nunique\" : session[\"event_count\"].nunique(),\n",
        "                \"last_game_timestamp\": timestamp,\n",
        "                \"last_game_world\": session_world,\n",
        "                \"last_game_session_title\": session_title\n",
        "            }\n",
        "    # for each assessment, and only this kind off session, the features below are processed and a register are generated\n",
        "    if (session_type == 'Assessment') & (test_set or len(session) > 1):\n",
        "      # search for event_code 4100, that represents the assessments trial\n",
        "      all_attempts = session.query(f'event_code == {event_data[\"win_code\"][session_title]}')\n",
        "\n",
        "      # then, check the numbers of wins and the number of losses\n",
        "      true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
        "      false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
        "\n",
        "      # copy a dict to use as feature template, it's initialized with some items:\n",
        "      # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
        "      # indexとデータをコピーしたものでデータフレームをアップデート\n",
        "      features = user_activities_count.copy()\n",
        "      features.update(last_accuracy_title.copy())\n",
        "      features.update(event_code_count.copy())\n",
        "      features.update(title_count.copy())\n",
        "      features.update(game_time_dict.copy())\n",
        "      features.update(event_id_count.copy())\n",
        "      features.update(title_event_code_count.copy())\n",
        "      features.update(assess_4020_acc_dict.copy())\n",
        "      features.update(type_world_count.copy())\n",
        "      features.update(last_game_time_title.copy())\n",
        "      features.update(ac_game_time_title.copy())\n",
        "      features.update(ac_true_attempts_title.copy())\n",
        "      features.update(ac_false_attempts_title.copy())\n",
        "\n",
        "      features.update(event_code_proc_count.copy())\n",
        "      features['installation_session_count'] = session_count\n",
        "      features['game_session'] =  game_session\n",
        "      features['timestamp'] =  timestamp\n",
        "      features['accumulated_game_miss'] = accumulated_game_miss\n",
        "      features['mean_game_round'] = mean_game_round\n",
        "      features['mean_game_duration'] = mean_game_duration\n",
        "      features['mean_game_level'] = mean_game_level\n",
        "      features['Assessment_mean_event_count'] = Assessment_mean_event_count\n",
        "      features['Game_mean_event_count'] = Game_mean_event_count\n",
        "      features['Activity_mean_event_count'] = Activity_mean_event_count\n",
        "      features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n",
        "\n",
        "      variety_features = [('var_event_code', event_code_count),\n",
        "                          ('var_event_id', event_id_count),\n",
        "                          ('var_title', title_count),\n",
        "                          ('var_title_event_code', title_event_code_count),\n",
        "                          ('var_type_world', type_world_count)]\n",
        "\n",
        "      for name, dict_counts in variety_features:\n",
        "                arr = np.array(list(dict_counts.values()))\n",
        "                features[name] = np.count_nonzero(arr)\n",
        "\n",
        "\n",
        "      # get installation_id for aggregated features\n",
        "      features['installation_id'] = session['installation_id'].iloc[-1]\n",
        "\n",
        "      # add title as feature, remembering that title represents the name of the game\n",
        "      features['session_title'] = session['title'].iloc[0]\n",
        "\n",
        "      # the 4 lines below add the feature of the history of the trials of this player\n",
        "      # this is based on the all time attempts so far, at the moment of this assessment\n",
        "      features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
        "      features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
        "      accumulated_correct_attempts += true_attempts\n",
        "      accumulated_uncorrect_attempts += false_attempts\n",
        "\n",
        "\n",
        "      # ----------------------------------------------\n",
        "      ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
        "      ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
        "\n",
        "      last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
        "      ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
        "      # ----------------------------------------------\n",
        "\n",
        "      # the time spent in the app so far\n",
        "      if durations == []:\n",
        "          features['duration_mean'] = np.nan\n",
        "          features['duration_std'] = np.nan\n",
        "          features['last_duration'] = np.nan\n",
        "          features['last-2_duration'] = np.nan\n",
        "          features['duration_max'] = np.nan\n",
        "          #features['duration_min'] = np.nan\n",
        "\n",
        "      else:\n",
        "          features['duration_mean'] = np.mean(durations)\n",
        "          features['duration_std'] = np.std(durations)\n",
        "          features['last_duration'] = durations[-1]\n",
        "          if len(durations)>1:\n",
        "              features['last-2_duration'] = durations[-2]\n",
        "          else:\n",
        "              features['last-2_duration'] = np.nan\n",
        "          features['duration_max'] = np.max(durations)\n",
        "          #features['duration_min'] = np.min(durations)\n",
        "\n",
        "      durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
        "\n",
        "      if durations_game == []:\n",
        "            features['duration_game_mean'] = np.nan\n",
        "            features['duration_game_std'] = np.nan\n",
        "            features['game_last_duration'] = np.nan\n",
        "            features['game_last-2_duration'] = np.nan\n",
        "            features['game_max_duration'] = np.nan\n",
        "            #features['game_min_duration'] = np.nan\n",
        "      else:\n",
        "            features['duration_game_mean'] = np.mean(durations_game)\n",
        "            features['duration_game_std'] = np.std(durations_game)\n",
        "            features['game_last_duration'] = durations_game[-1]\n",
        "            if len(durations_game)>1:\n",
        "                features['game_last-2_duration'] = durations_game[-2]\n",
        "            else:\n",
        "                features['game_last-2_duration'] = np.nan\n",
        "            features['game_max_duration'] = np.max(durations_game)\n",
        "            #features['game_min_duration'] = np.min(durations_game)\n",
        "\n",
        "      if last_Game_Features  == {}:\n",
        "            features[\"last_game_event_count\"] = np.nan\n",
        "            features[\"last_game_event_count_nunique\"] = np.nan\n",
        "            features[\"last_game_timestamp\"] = np.nan\n",
        "            features[\"last_game_world_is_the_same\"] = np.nan\n",
        "            features[\"last_game_session_title\"] = np.nan\n",
        "      else:\n",
        "            features[\"last_game_event_count\"] =  last_Game_Features[\"last_game_event_count\"]\n",
        "            features[\"last_game_event_count_nunique\"] =  last_Game_Features[\"last_game_event_count_nunique\"]\n",
        "            features[\"last_game_timestamp\"] = last_Game_Features[\"last_game_timestamp\"]\n",
        "            features[\"last_game_world_is_the_same\"] = int(last_Game_Features[\"last_game_world\"] == session_world)\n",
        "            features[\"last_game_session_title\"] = last_Game_Features[\"last_game_session_title\"]\n",
        "\n",
        "        \n",
        "      if last_Assessment_Features  == {}:\n",
        "            features[\"last_assessment_event_count\"] = np.nan\n",
        "            features[\"last_assessment_event_count_nunique\"] = np.nan\n",
        "            features[\"last_assessment_timestamp\"] = np.nan\n",
        "            features[\"last_assessment_world_is_the_same\"] = np.nan\n",
        "            features[\"last_assessment_title_is_the_same\"] = np.nan\n",
        "            features[\"last_assessment_accuracy_group\"] = np.nan\n",
        "            features[\"last_assessment_session_title\"] = np.nan\n",
        "            \n",
        "      else:\n",
        "            features[\"last_assessment_event_count\"] =  last_Assessment_Features[\"last_assessment_event_count\"]\n",
        "            features[\"last_assessment_event_count_nunique\"] =  last_Assessment_Features[\"last_assessment_event_count_nunique\"]\n",
        "            features[\"last_assessment_timestamp\"] = last_Assessment_Features[\"last_assessment_timestamp\"]\n",
        "            features[\"last_assessment_world_is_the_same\"] = int(last_Assessment_Features[\"last_assessment_world\"] == session_world)\n",
        "            features[\"last_assessment_title_is_the_same\"] = int(last_Assessment_Features[\"last_assessment_title\"] == session_title)\n",
        "            features[\"last_assessment_accuracy_group\"] = last_Assessment_Features[\"last_assessment_accuracy_group\"]\n",
        "            features[\"last_assessment_session_title\"] = last_Assessment_Features[\"last_assessment_title\"]\n",
        "\n",
        "\n",
        "      if last_Activity_Features  == {}:\n",
        "            features[\"last_activity_event_count\"] = np.nan\n",
        "            features[\"last_activity_event_count_nunique\"] = np.nan\n",
        "            features[\"last_activity_timestamp\"] = np.nan\n",
        "            features[\"last_activity_world_is_the_same\"] = np.nan\n",
        "            features[\"last_activity_session_title\"] = np.nan\n",
        "      else:\n",
        "            features[\"last_activity_event_count\"] =  last_Activity_Features[\"last_activity_event_count\"]\n",
        "            features[\"last_activity_event_count_nunique\"] =  last_Activity_Features[\"last_activity_event_count_nunique\"]\n",
        "            features[\"last_activity_timestamp\"] = last_Activity_Features[\"last_activity_timestamp\"]\n",
        "            features[\"last_activity_world_is_the_same\"] = int(last_Activity_Features[\"last_activity_world\"] == session_world)\n",
        "            features[\"last_activity_session_title\"] = last_Activity_Features[\"last_activity_session_title\"]\n",
        "\n",
        "        \n",
        "      if durations_activity == []:\n",
        "            #features['duration_activity_mean'] = np.nan\n",
        "            #features['duration_activity_std'] = np.nan\n",
        "            #features['activity_last_duration'] = np.nan\n",
        "            #features['activity_last-2_duration'] = np.nan\n",
        "            #features['activity_max_duration'] = np.nan\n",
        "            features['activity_min_duration'] = np.nan\n",
        "      else:\n",
        "            #features['duration_activity_mean'] = np.mean(durations_activity)\n",
        "            #features['duration_activity_std'] = np.std(durations_activity)\n",
        "            #features['activity_last_duration'] = durations_activity[-1]\n",
        "            #if len(durations_activity)>1:\n",
        "            #    features['activity_last-2_duration'] = durations_activity[-2]\n",
        "            #else:\n",
        "            #    features['activity_last-2_duration'] = np.nan\n",
        "            #features['activity_max_duration'] = np.max(durations_activity)\n",
        "            features['activity_min_duration'] = np.min(durations_activity)\n",
        "\n",
        "      # the accuracy is the all time wins divided by the all time attempts\n",
        "      features['accumulated_accuracy'] = accumulated_accuracy / counter if counter > 0 else 0\n",
        "\n",
        "      # --------------------------\n",
        "      features['Cauldron_Filler_4025'] = Cauldron_Filler_4025 / counter if counter > 0 else 0\n",
        "\n",
        "      Assess_4025 = session[(session.event_code == 4025) & (session.title == 'Cauldron Filler (Assessment)')]\n",
        "      true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n",
        "      false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n",
        "\n",
        "      cau_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (true_attempts_ + false_attempts_) != 0 else 0\n",
        "      Cauldron_Filler_4025 += cau_assess_accuracy_\n",
        "\n",
        "      chest_assessment_uncorrect_sum += len(session[session.event_id == \"df4fe8b6\"])\n",
        "\n",
        "      Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n",
        "      # ----------------------------\n",
        "\n",
        "      accuracy = true_attempts / (true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n",
        "      accumulated_accuracy += accuracy\n",
        "      last_accuracy_title['acc_' + session_title_text] = accuracy\n",
        "\n",
        "      # a feature of the current accuracy categorized\n",
        "      # it is a counter of how many times this player was in each accuracy group\n",
        "      if accuracy == 0:\n",
        "          features['accuracy_group'] = 0\n",
        "      elif accuracy == 1:\n",
        "          features['accuracy_group'] = 3\n",
        "      elif accuracy == 0.5:\n",
        "          features['accuracy_group'] = 2\n",
        "      else:\n",
        "          features['accuracy_group'] = 1\n",
        "      features.update(accuracy_groups)\n",
        "\n",
        "      accuracy_groups[features['accuracy_group']] += 1\n",
        "\n",
        "      # mean of the all accuracy groups of this player\n",
        "      features['accumulated_accuracy_group'] = accumulated_accuracy_group / counter if counter > 0 else 0\n",
        "\n",
        "      accumulated_accuracy_group += features['accuracy_group']\n",
        "\n",
        "      # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
        "      features['accumulated_actions'] = accumulated_actions\n",
        "\n",
        "      features[\"acc_current_assessment\"] = features[\"acc_\"+session_title_text]\n",
        "      features[\"lgt_current_assessment\"] = features[\"lgt_\"+session_title_text]\n",
        "      features[\"agt_current_assessment\"] = features[\"agt_\"+session_title_text]\n",
        "      features[\"ata_current_assessment\"] = features[\"ata_\"+session_title_text]\n",
        "      features[\"afa_current_assessment\"] = features[\"afa_\"+session_title_text]\n",
        "\n",
        "      features[\"current_4020_accuracy\"] = features.get(session_title_text + \"_4020_accuracy\",-1)\n",
        "\n",
        "\n",
        "      features[\"last_game_session_correct_true\"] = last_game_session_correct_true\n",
        "      features[\"last_game_session_correct_false\"] = last_game_session_correct_false\n",
        "      features[\"last_2_game_session_correct_true\"] = last_2_game_session_correct_true\n",
        "      features[\"last_2_game_session_correct_false\"] = last_2_game_session_correct_false\n",
        "      \n",
        "      features[\"acc_game_session_correct_true\"] = acc_game_session_correct_true\n",
        "      features[\"acc_game_session_correct_false\"] = acc_game_session_correct_false\n",
        "      \n",
        "      session_type_story_length = len(session_type_story)\n",
        "      features[\"session_type_story_length\"] = session_type_story_length\n",
        "      features[\"session_type_story_count_clip\"] =session_type_story.count('Clip')\n",
        "      features[\"session_type_story_count_game\"] =session_type_story.count('Game')\n",
        "      features[\"session_type_story_count_activity\"] =session_type_story.count('Activity')\n",
        "      features[\"session_type_story_count_assessment\"] =session_type_story.count('Assessment')\n",
        "      \n",
        "      session_type_story = []\n",
        "      \n",
        "      last_Assessment_Features = {\n",
        "          \"last_assessment_event_count\":session[\"event_count\"].values[-1],\n",
        "          \"last_assessment_event_count_nunique\" : session[\"event_count\"].nunique(),\n",
        "          \"last_assessment_timestamp\": timestamp,\n",
        "          \"last_assessment_world\": session_world,\n",
        "          \"last_assessment_title\": session_title,\n",
        "          \"last_assessment_accuracy_group\": features['accuracy_group'] \n",
        "      }\n",
        "      \n",
        "      # there are some conditions to allow this features to be inserted in the datasets\n",
        "      # if it's a test set, all sessions belong to the final dataset\n",
        "      # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
        "      # that means, must exist an event_code 4100 or 4110\n",
        "      if test_set:\n",
        "          last_assesment = features.copy()\n",
        "\n",
        "      if true_attempts + false_attempts > 0:\n",
        "          all_assessments.append(features)\n",
        "\n",
        "          \n",
        "      counter += 1\n",
        "\n",
        "    session_count += 1\n",
        "\n",
        "    # this piece counts how many actions was made in each event_code so far\n",
        "    def update_counters(counter: dict, col: str):\n",
        "        num_of_session_count = Counter(session[col])\n",
        "        for k in num_of_session_count.keys():\n",
        "            x = k\n",
        "            if col == 'title':\n",
        "                x = event_data[\"activities_labels\"][k]\n",
        "            counter[x] += num_of_session_count[k]\n",
        "        return counter\n",
        "\n",
        "    def update_proc(count: dict):\n",
        "        res = {}\n",
        "        for k, val in count.items():\n",
        "            res[str(k) + \"_proc\"] = (float(val) * 100.0) / accumulated_actions\n",
        "        return res\n",
        "\n",
        "    event_code_count = update_counters(event_code_count, \"event_code\")\n",
        "\n",
        "    event_id_count = update_counters(event_id_count, \"event_id\")\n",
        "    title_count = update_counters(title_count, 'title')\n",
        "    title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
        "    type_world_count = update_counters(type_world_count, 'type_world')\n",
        "\n",
        "    assess_4020_acc_dict = get_4020_acc(session, assess_4020_acc_dict, event_data)\n",
        "    game_time_dict[session_type + '_gametime'] = (game_time_dict[session_type + '_gametime'] + (\n",
        "                    session['game_time'].iloc[-1] / 1000.0)) / 2.0\n",
        "\n",
        "    # counts how many actions the player has done so far, used in the feature of the same name\n",
        "    accumulated_actions += len(session)\n",
        "    event_code_proc_count = update_proc(event_code_count)\n",
        "\n",
        "    if last_activity != session_type:\n",
        "        user_activities_count[session_type] += 1\n",
        "        last_activitiy = session_type\n",
        "\n",
        "    session_type_story.append ( session_type ) \n",
        "\n",
        "  # if it't the test_set, only the last assessment must be predicted, the previous goes to the dataset\n",
        "  if test_set:\n",
        "      return last_assesment, all_assessments\n",
        "  # in the train_set, all assessments goes to the dataset\n",
        "  return all_assessments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBQOmE2n3TC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnt_miss(df):\n",
        "    cnt = 0\n",
        "    for e in range(len(df)):\n",
        "        x = df['event_data'].iloc[e]\n",
        "        y = json.loads(x)['misses']\n",
        "        cnt += y\n",
        "    return cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCMZT6SI1ac6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_4020_acc(df, counter_dict, event_data):\n",
        "  for e in ['Cauldron Filler (Assessment)', 'Bird Measurer (Assessment)',\n",
        "            'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)']:\n",
        "      Assess_4020 = df[(df.event_code == 4020) & (df.title == event_data[\"activities_map\"][e])]\n",
        "      true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n",
        "      false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n",
        "\n",
        "      measure_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n",
        "                                                                                                    true_attempts_ + false_attempts_) != 0 else 0\n",
        "      counter_dict[e + \"_4020_accuracy\"] += (counter_dict[e + \"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n",
        "\n",
        "  return counter_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOtZx7_k9lxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_users_data(users_list, return_dict,  event_data, test_set):\n",
        "  if test_set:\n",
        "      for user in users_list:\n",
        "          return_dict.append(get_data(user, event_data, test_set))\n",
        "  else:\n",
        "      answer = []\n",
        "      for user in users_list:\n",
        "          answer += get_data(user, event_data, test_set)\n",
        "      return_dict += answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PXfAnd49uou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_and_test_single_proc(train, test, event_data, load_train = False):\n",
        "  if load_train :\n",
        "      reduce_train = pd.read_pickle (PROCESSED_TRAIN_PATH)\n",
        "  else:    \n",
        "      compiled_train = []\n",
        "      for ins_id, user_sample in tqdm(train.groupby('installation_id', sort=False), total=17000):\n",
        "          compiled_train += get_data(user_sample, event_data, False)\n",
        "      reduce_train = pd.DataFrame(compiled_train)\n",
        "  \n",
        "  compiled_test = []\n",
        "  compiled_test_to_train = []\n",
        "  for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n",
        "      test_data = get_data(user_sample, event_data, True)\n",
        "      compiled_test.append(test_data[0])\n",
        "      compiled_test_to_train += test_data[1]\n",
        "\n",
        "\n",
        "  reduce_test = pd.DataFrame(compiled_test)\n",
        "  reduce_test_to_train = pd.DataFrame(compiled_test_to_train)\n",
        "\n",
        "  return reduce_train, reduce_test, reduce_test_to_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2hQvm9g987G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 相関係数が高いものを除外\n",
        "def remove_correlated_features(reduce_train,features ):\n",
        "  counter = 0\n",
        "  to_remove = []\n",
        "  for feat_a in features:\n",
        "      for feat_b in features:\n",
        "          if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
        "              c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
        "              if c > 0.9999:\n",
        "                  counter += 1\n",
        "                  to_remove.append(feat_b)\n",
        "                  print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n",
        "  return to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safyQ74V-BT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_process (df):\n",
        "  df[\"hour\"]=df[\"timestamp\"].dt.hour\n",
        "  df[\"dayofyear\"]=df[\"timestamp\"].dt.dayofyear\n",
        "  df[\"dayofweek\"]=df[\"timestamp\"].dt.dayofweek\n",
        "  df['sin_hour'] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
        "  df['cos_hour'] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
        "  df = df.drop ([\"hour\"], axis = 1)  \n",
        "  \n",
        "  df[\"timestamp\"]=df[\"timestamp\"].astype(int)\n",
        "  df[\"last_game_timestamp\"]=(df[\"timestamp\"] - df[\"last_game_timestamp\"].astype(int)) // 1e6\n",
        "  df[\"last_assessment_timestamp\"]=(df[\"timestamp\"] - df[\"last_assessment_timestamp\"].astype(int)) // 1e6\n",
        "  df[\"last_activity_timestamp\"]=(df[\"timestamp\"] - df[\"last_activity_timestamp\"].astype(int)) // 1e6\n",
        "  \n",
        "  df[\"last_game_session_correct_accuracy\"] = df[\"last_game_session_correct_true\"]/(df[\"last_game_session_correct_true\"]+df[\"last_game_session_correct_false\"])\n",
        "  df[\"last_2_game_session_correct_accuracy\"] = df[\"last_2_game_session_correct_true\"]/(df[\"last_2_game_session_correct_true\"]+df[\"last_2_game_session_correct_false\"])\n",
        "  df[\"last_game_session_correct_accuracy*last_2_game_session_correct_accuracy\"] = df[\"last_game_session_correct_accuracy\"]*df[\"last_2_game_session_correct_accuracy\"]\n",
        "  \n",
        "  df[\"acc_game_session_correct_accuracy\"] = df[\"acc_game_session_correct_true\"]/(df[\"acc_game_session_correct_true\"]+df[\"acc_game_session_correct_false\"])\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5r7XkjT-WfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_generator (X, y, groups):\n",
        "  X = X.copy()\n",
        "  X[\"__y__\"] = y\n",
        "  X[\"__groups__\"] = groups\n",
        "  state = 44\n",
        "  while True:\n",
        "      X = X.sample(frac=1.0, random_state=state)\n",
        "      state +=1\n",
        "      _X = X.drop_duplicates([\"__groups__\"], keep=\"first\")\n",
        "      yield _X.drop([\"__y__\",\"__groups__\"],axis=1), _X[\"__y__\"].values, _X[\"__groups__\"].values  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlgBNPIm-jhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trunc_rmse_qwk_score(y,pred,groups, nsamples=5000):\n",
        "  rmse_scores = np.zeros ( (nsamples, ))\n",
        "  oof_cohen_scores = np.zeros ( (nsamples, ))\n",
        "  \n",
        "  X=pd.DataFrame()\n",
        "  X[\"pred\"] = pred\n",
        "  truncate_gen = truncate_generator (X, y, groups)\n",
        "  for i in range (nsamples):\n",
        "      X, y, groups = truncate_gen.__next__()\n",
        "      pred = X[\"pred\"]\n",
        "      rmse_scores [i]= rmse(y, pred)\n",
        "      oof_cohen_scores [i]= cohen_kappa_score(y, eval_qwk_lgb_regr(pred, y), weights = 'quadratic')    \n",
        "  \n",
        "  return rmse_scores, oof_cohen_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dQpEaEG-pVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trunc_qwk_score(y,pred,groups, nsamples=5000):\n",
        "  oof_cohen_scores = np.zeros( (nsamples, ))\n",
        "  \n",
        "  X=pd.DataFrame()\n",
        "  X[\"pred\"] = pred\n",
        "  truncate_gen = truncate_generator (X, y, groups)\n",
        "  for i in range (nsamples):\n",
        "      X, y, groups = truncate_gen.__next__()\n",
        "      pred = X[\"pred\"]\n",
        "      oof_cohen_scores [i]= qwk3(pred, y)    \n",
        "  \n",
        "  return oof_cohen_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow11UGqj-yx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trunc_auc_score(y,pred,groups, nsamples=5000):\n",
        "    scores = np.zeros ( (nsamples, ))\n",
        "    \n",
        "    X=pd.DataFrame()\n",
        "    X[\"pred\"] = pred\n",
        "    truncate_gen = truncate_generator (X, y, groups)\n",
        "    for i in range (nsamples):\n",
        "        X, y, groups = truncate_gen.__next__()\n",
        "        pred = X[\"pred\"]\n",
        "        scores [i]= roc_auc_score(y,pred)    \n",
        "    \n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCRWxX3-7I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oof_lgb (X, ids, y, groups, num_boost_round, early_stopping_rounds, params, categoricals, splits, verbose_eval):\n",
        "  columns = [c for c in X.columns]\n",
        "  \n",
        "  oof = pd.DataFrame({\"id\":ids})\n",
        "  y_oof = np.zeros((X.shape[0], ))\n",
        "  \n",
        "  models= []\n",
        "  \n",
        "\n",
        "  feature_importances = pd.DataFrame()\n",
        "  feature_importances['feature'] = columns \n",
        "\n",
        "  for fold_n, (train_index, valid_index) in enumerate(splits):\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    #group_train = groups.iloc [train_index] \n",
        "    #train_truncate_gen = truncate_generator ( X_train, y_train, group_train)\n",
        "\n",
        "    \n",
        "    group_valid = groups.iloc [valid_index] \n",
        "    \n",
        "    truncate_gen = truncate_generator ( X_valid, y_valid, group_valid) \n",
        "    \n",
        "    X_valid_trunc, y_valid_trunc, _ = truncate_gen.__next__()\n",
        "    \n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dvalid = lgb.Dataset(X_valid_trunc, label=y_valid_trunc)\n",
        "\n",
        "    model = lgb.train(params, dtrain, num_boost_round, \n",
        "                    valid_sets = [dtrain, dvalid],\n",
        "                    categorical_feature = categoricals,\n",
        "                    verbose_eval=verbose_eval, early_stopping_rounds=early_stopping_rounds)\n",
        "\n",
        "    y_pred = model.predict(X_valid)\n",
        "\n",
        "    #if verbose_eval > 0:\n",
        "    #   rmse_scores, oof_cohen_scores = trunc_rmse_qwk_score (y_valid.values, y_pred, group_valid.values,nsamples=5000)\n",
        "    #    print (f'fold: {fold_n}, rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}' )\n",
        "    \n",
        "    models.append(model)\n",
        "\n",
        "    feature_importances[f'fold_{fold_n + 1}'] = model.feature_importance()\n",
        "\n",
        "    y_oof[valid_index] = y_pred\n",
        "\n",
        "    del X_train, X_valid, y_train, y_valid\n",
        "    gc.collect()\n",
        "\n",
        "  feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(len(splits))]].mean(axis=1)\n",
        "  feature_importances = feature_importances.sort_values (by=\"average\", ascending = False)\n",
        "  feature_importances.to_csv(\"feature_importance.csv\", index=False)\n",
        "  return models, y_oof"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj8SzQS2_j0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lgb (df_train, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds, verbose_eval  ):\n",
        "  if verbose_eval > 0:\n",
        "    print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n",
        "    \n",
        "  X = df_train[model_feats]\n",
        "  y = df_train[\"accuracy_group\"]\n",
        "  ids = df_train[\"sample\"]\n",
        "  groups = df_train[\"installation_id\"]\n",
        "\n",
        "  folds = GroupKFold(n_splits=nfolds) \n",
        "\n",
        "  splits = []\n",
        "  for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n",
        "      splits.append((train_index, valid_index)) \n",
        "\n",
        "  lgb_model, y_oof = oof_lgb (X, ids, y, groups,  num_boost_round, early_stopping_rounds, params, categoricals=categorical_features, splits=splits, verbose_eval = verbose_eval)\n",
        "  \n",
        "  return lgb_model, y_oof  #, rmse_scores, oof_cohen_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0KLmR4g_vIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n",
        "  y_oof = np.zeros ( (df_train.shape[0],) )\n",
        "  for i in range(nmodels):\n",
        "      lgb_i_models, y_i_oof  = run_lgb(df_train, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n",
        "      y_oof += y_i_oof \n",
        "      for n,model in enumerate(lgb_i_models):\n",
        "          model.save_model(f'lgb_{i}_model_fold_{n}.txt')\n",
        "      params['seed'] +=1\n",
        "      params['feature_fraction_seed'] +=1\n",
        "      params['bagging_seed'] +=1\n",
        "      params['drop_seed'] +=1\n",
        "      params['data_random_seed'] +=1            \n",
        "  \n",
        "  y_oof = y_oof / nmodels\n",
        "  \n",
        "  rmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n",
        "  print (f'lgb {nmodels} models rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n",
        "  \n",
        "  return y_oof, rmse_scores,  oof_cohen_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhaElFl4_8vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lgb_auc (df_train, y, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds, verbose_eval  ):\n",
        "  if verbose_eval > 0:\n",
        "      print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n",
        "  \n",
        "  X = df_train[model_feats]\n",
        "\n",
        "  ids = df_train[\"sample\"]\n",
        "  groups = df_train[\"installation_id\"]\n",
        "\n",
        "  folds = GroupKFold(n_splits=nfolds) \n",
        "\n",
        "  splits = []\n",
        "  for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n",
        "      splits.append((train_index, valid_index)) \n",
        "\n",
        "  lgb_model, y_oof = oof_lgb (X, ids, y, groups,  num_boost_round, early_stopping_rounds, params, categoricals=categorical_features, splits=splits, verbose_eval = verbose_eval)\n",
        "  \n",
        "  return lgb_model, y_oof  #, rmse_scores, oof_cohen_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqfP4LF1AEIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lgb_auc_ensemble (df_train, y, name,  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n",
        "  y_oof = np.zeros ( (df_train.shape[0],) )\n",
        "  for i in range(nmodels):\n",
        "      lgb_i_models, y_i_oof  = run_lgb_auc(df_train, y, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n",
        "      y_oof += y_i_oof \n",
        "      for n,model in enumerate(lgb_i_models):\n",
        "          model.save_model(f'lgb_{name}_{i}_model_fold_{n}.txt')\n",
        "      params['seed'] +=1\n",
        "      params['feature_fraction_seed'] +=1\n",
        "      params['bagging_seed'] +=1\n",
        "      params['drop_seed'] +=1\n",
        "      params['data_random_seed'] +=1            \n",
        "  \n",
        "  y_oof = y_oof / nmodels\n",
        "  \n",
        "  auc_scores  = trunc_auc_score ( y.values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n",
        "  print (f'lgb {nmodels} models auc: {np.median(auc_scores)} std:{auc_scores.std()}')\n",
        "  \n",
        "  return y_oof, auc_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGaUTthMAI1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oof_xgb (X, ids, y, groups,num_boost_round, early_stopping_rounds, params, splits, verbose_eval):\n",
        "  columns = [c for c in X.columns]\n",
        "  \n",
        "  oof = pd.DataFrame({\"id\":ids})\n",
        "  y_oof = np.zeros((X.shape[0], ))\n",
        "  \n",
        "  models= []\n",
        "  score = 0\n",
        "  for fold_n, (train_index, valid_index) in enumerate(splits):\n",
        "      X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "      y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "      group_valid = groups.iloc [valid_index] \n",
        "      \n",
        "      truncate_gen = truncate_generator ( X_valid, y_valid, group_valid) \n",
        "      \n",
        "      X_valid_trunc, y_valid_trunc, _ = truncate_gen.__next__()\n",
        "    \n",
        "  \n",
        "      dtrain = xgb.DMatrix(X_train,label=y_train)\n",
        "      dvalid = xgb.DMatrix(X_valid_trunc,label=y_valid_trunc)\n",
        "      \n",
        "      model = xgb.train(params, dtrain=dtrain, num_boost_round=num_boost_round, evals =  [(dtrain, 'train'),(dvalid, 'valid')],\n",
        "                        early_stopping_rounds=early_stopping_rounds, maximize=False, verbose_eval=verbose_eval)\n",
        "\n",
        "\n",
        "      dvalid = xgb.DMatrix(X_valid,label=y_valid)\n",
        "      y_pred = model.predict(dvalid)\n",
        "      \n",
        "          \n",
        "          \n",
        "      models.append(model)\n",
        "\n",
        "      y_oof[valid_index] = y_pred\n",
        "\n",
        "      del X_train, X_valid, y_train, y_valid\n",
        "      gc.collect()\n",
        "\n",
        "  #rmse_scores, oof_cohen_scores = trunc_rmse_qwk_score (y, y_oof, groups)\n",
        "  #avg_score = score/len(splits)    \n",
        "  #if verbose_eval > 0:\n",
        "  #    print (f'oof rmse: {rmse_scores.median()}, mean rmse: {avg_score}, cohen kappa score : {oof_cohen_scores.median()}')\n",
        "  \n",
        "  return models, y_oof"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbEsaxpaAXk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_xgb (df_train, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds  , verbose_eval   ):\n",
        "  if verbose_eval > 0:\n",
        "      print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n",
        "  \n",
        "  X = df_train[model_feats].copy()\n",
        "  \n",
        "  X = pd.get_dummies(X, dummy_na=True, columns=categorical_features)\n",
        "  \n",
        "  y = df_train[\"accuracy_group\"]\n",
        "  ids = df_train[\"sample\"]\n",
        "  groups = df_train[\"installation_id\"]\n",
        "\n",
        "  folds = GroupKFold(n_splits=nfolds) \n",
        "\n",
        "  splits = []\n",
        "  for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n",
        "      splits.append((train_index, valid_index)) \n",
        "\n",
        "  xgb_model, y_oof = oof_xgb (X, ids, y, groups, num_boost_round, early_stopping_rounds, params,  splits=splits, verbose_eval = verbose_eval)\n",
        "  \n",
        "  return xgb_model, y_oof"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxK5RCqjAlLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_xgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n",
        "  y_oof = np.zeros ( (df_train.shape[0],) )\n",
        "  for i in range(nmodels):\n",
        "      xgb_i_models, y_i_oof  = run_xgb(df_train, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n",
        "      y_oof += y_i_oof \n",
        "      for n,model in enumerate(xgb_i_models):\n",
        "          model.save_model(f'xgb_{i}_model_fold_{n}.txt')\n",
        "      params['seed'] +=1       \n",
        "  \n",
        "  y_oof = y_oof / nmodels\n",
        "  \n",
        "  rmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n",
        "  print (f'xgb {nmodels} models rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n",
        "  \n",
        "  return y_oof, rmse_scores,  oof_cohen_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqT88suDAuWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_lgb (nmodel, nfolds):\n",
        "  y_blend = np.zeros((test.shape[0], ))     \n",
        "  \n",
        "  for i in range(nmodels):\n",
        "      y_test = np.zeros((test.shape[0], ))     \n",
        "\n",
        "      for n in range(nfolds):\n",
        "          model_file = f'./lgb_{i}_model_fold_{n}.txt'\n",
        "          model = lgb.Booster(model_file=model_file)\n",
        "          pred = model.predict( test[model_feats], num_iteration=model.best_iteration )\n",
        "          y_test += pred\n",
        "      \n",
        "      y_blend += y_test / nfolds\n",
        "  \n",
        "  y_blend = y_blend / nmodels\n",
        "  \n",
        "  return y_blend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_zNTlusA0I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_auc_lgb (name, nmodel, nfolds):\n",
        "  y_blend = np.zeros((test.shape[0], ))     \n",
        "  \n",
        "  for i in range(nmodels):\n",
        "      y_test = np.zeros((test.shape[0], ))     \n",
        "\n",
        "      for n in range(nfolds):\n",
        "          model_file = f'./lgb_{name}_{i}_model_fold_{n}.txt'\n",
        "          model = lgb.Booster(model_file=model_file)\n",
        "          pred = model.predict( test[model_feats], num_iteration=model.best_iteration )\n",
        "          y_test += pred\n",
        "      \n",
        "      y_blend += y_test / nfolds\n",
        "  \n",
        "  y_blend = y_blend / nmodels\n",
        "  \n",
        "  return y_blend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7Z3BaxA3NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_xgb (nmodel, nfolds):\n",
        "  y_blend = np.zeros((test.shape[0], ))     \n",
        "\n",
        "  test_xgb = test[model_feats].copy()\n",
        "  test_xgb = pd.get_dummies(test_xgb,  dummy_na=True, columns=categorical_features)\n",
        "  \n",
        "  for i in range(nmodels):\n",
        "      y_test = np.zeros((test.shape[0], ))     \n",
        "\n",
        "      for n in range(nfolds):\n",
        "          model_file = f'./xgb_{i}_model_fold_{n}.txt'\n",
        "          model = xgb.Booster() #init model\n",
        "          model.load_model(model_file) # load data\n",
        "          dtest =  xgb.DMatrix (test_xgb)\n",
        "          pred = model.predict(dtest)           \n",
        "          y_test += pred\n",
        "      \n",
        "      y_blend += y_test / nfolds\n",
        "  \n",
        "  y_blend = y_blend / nmodels\n",
        "  \n",
        "  return y_blend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQpWdo7yA8Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sample_submission, y_pred):\n",
        "  sample_submission['accuracy_group'] = y_pred\n",
        "  sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n",
        "  sample_submission.to_csv('submission.csv', index = False)\n",
        "  print(sample_submission['accuracy_group'].value_counts(normalize = True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In6wDDyPA-df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_bound ( y_oof, groups ):\n",
        "  dist = Counter(groups)\n",
        "  for k in dist:\n",
        "      dist[k] /= len(y_oof)\n",
        "\n",
        "  acum = 0\n",
        "  bound = {}\n",
        "  for i in range(3):\n",
        "      acum += dist[i]\n",
        "      bound[i] = np.percentile(y_oof, acum * 100)\n",
        "\n",
        "  return bound[0], bound[1] - bound[0],  bound[2] - bound[1]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp8kuqDwBFKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def qwk3_optimizer ( y, y_oof, init_points = 20, n_iter = 50  ):\n",
        "  pbounds = {'a': (0.0, 1.5), 'b': (0.0, 1.5), 'c': (0.0, 1.5)}\n",
        "\n",
        "  def qwk3_opt ( x, y, a,b,c ):\n",
        "\n",
        "      x = round_prediction ( x, a,b,c )\n",
        "\n",
        "      return qwk3 ( x, y, max_rat=3 )\n",
        "\n",
        "\n",
        "  def q (a,b,c):\n",
        "      return qwk3_opt  ( y_oof, y, a,b,c )\n",
        "\n",
        "  optimizer = BayesianOptimization(\n",
        "      f=q,\n",
        "      pbounds=pbounds,\n",
        "      random_state=44,\n",
        "  )\n",
        "\n",
        "\n",
        "  optimizer.maximize(\n",
        "      init_points=init_points,\n",
        "      n_iter=n_iter,\n",
        "  )\n",
        "\n",
        "  a = optimizer.max[\"params\"][\"a\"]\n",
        "  b = optimizer.max[\"params\"][\"b\"]\n",
        "  c = optimizer.max[\"params\"][\"c\"]\n",
        "  t = optimizer.max[\"target\"]\n",
        "  print ( f'qwk3:{t}, a:{a}, b:{b}, c:{c}' )\n",
        "  \n",
        "  return a,b,c "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZeyIs4EBF5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "c6bd7a6e-b091-458d-fb9e-78ab746bfce0"
      },
      "source": [
        "NFOLDS = 5\n",
        "NMODELS = 5\n",
        "\n",
        "event_data = {}\n",
        "\n",
        "# read data\n",
        "train, test, train_labels, specs, sample_submission = read_data()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading train.csv file....\n",
            "Training.csv file have 11341042 rows and 11 columns\n",
            "Reading test.csv file....\n",
            "Test.csv file have 1156414 rows and 11 columns\n",
            "Reading train_labels.csv file....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-91826a1cddcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-1fc85eff3d9d>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading train_labels.csv file....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train_labels.csv file have {} rows and {} columns'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/train_labels.csv' does not exist: b'/content/train_labels.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GgbsWh-B0gp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "6e233846-2bc7-4aec-8313-f4125beb715c"
      },
      "source": [
        "# get usefull dict with maping encode\n",
        "train, test, event_data_update = encode_title(train, test, train_labels)\n",
        "event_data.update(event_data_update)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-509074e74ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMIVy30ZB27R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "59eaf522-e59c-4489-af36-f005d1584086"
      },
      "source": [
        "reduce_train, reduce_test, reduce_train_from_test = get_train_and_test_single_proc(train, test, event_data, load_train=False)\n",
        "\n",
        "reduce_train = post_process(reduce_train)\n",
        "reduce_test = post_process(reduce_test)\n",
        "reduce_train_from_test = post_process(reduce_train_from_test)\n",
        "reduce_train.shape,reduce_test.shape,reduce_train_from_test.shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-a0450583a9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduce_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_train_from_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_and_test_single_proc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreduce_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreduce_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreduce_train_from_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_train_from_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GST4-EauB3B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vS1L8-BGA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete train and test to release memory\n",
        "del train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F1YNHD-BXeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03jM7LpjBXj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_train = reduce_train.append (reduce_train_from_test, sort = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkPfwkgfBXs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2qbqr9BXqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = reduce_test\n",
        "df_train = reduce_train\n",
        "\n",
        "df_train[\"sample\"] = df_train[\"installation_id\"] + \"_\" + df_train[\"game_session\"]\n",
        "test[\"sample\"]=test[\"installation_id\"]\n",
        "\n",
        "\n",
        "model_feats = [\n",
        "'last_game_session_title', 'session_title', '4070_proc', 'acc_game_session_correct_accuracy', 'last_game_session_correct_accuracy',\n",
        "'4020_proc', 'activity_min_duration', '2030_proc', 'last_game_timestamp', 'acc_current_assessment', '3021_proc', 'ata_current_assessment',\n",
        "'2000_proc', 'last_2_game_session_correct_accuracy', '2010_proc', 'last_assessment_timestamp', 'last_game_event_count', 'Activity_gametime',\n",
        "'4025_proc', 'last_activity_event_count', 'accumulated_accuracy', 'duration_mean', 'last_duration', '4090_proc', 'Clip', 'last_assessment_event_count',\n",
        "'game_last_duration', 'game_last-2_duration', 'accumulated_accuracy_group', '3020_proc', '4040_proc', '4035_proc', 'afa_current_assessment',\n",
        "'mean_game_duration', 'Activity_mean_event_count', '4030_proc', '4010_proc', '4100_proc', 'duration_std', 'duration_game_mean',\n",
        "'last_assessment_session_title', '2035_proc', 'sin_hour', '3120_proc', '3110_proc', 'lgt_Cauldron Filler (Assessment)', '2025_proc',\n",
        "'2080_proc', '3010_proc', '4021_proc', 'session_type_story_count_clip', 'agt_Cart Balancer (Assessment)', '4095_proc', 'Game_mean_event_count',\n",
        "'2075_proc', 'game_max_duration', '2060_proc', '7372e1a5', 'duration_game_std', '56817e2b', 2000, 'Assessment_gametime', 'mean_game_round',\n",
        "'15a43e5b', '2083_proc', 'var_title', '3ee399c3', 'cos_hour', 'last_2_game_session_correct_true', 'duration_max', 4035, '4031_proc', '5000_proc',\n",
        "'4045_proc', 'agt_Cauldron Filler (Assessment)', 'dayofweek', 'bbfe0445', '4220_proc', 'b120f2ac', 'e694a35b', 'lgt_Chest Sorter (Assessment)',\n",
        "'587b5989', '84538528', 'last_game_session_correct_true', 'current_4020_accuracy', '6bf9e3e1', 'Sandcastle Builder (Activity)', '3afde5dd',\n",
        "'last_assessment_event_count_nunique', 'last_activity_event_count_nunique', '2081_proc', 3010, 3021, 3020, 'Mushroom Sorter (Assessment)_4020_accuracy',\n",
        "'51102b85', '1bb5fbdb', '4110_proc', 'accumulated_uncorrect_attempts', 'Bird Measurer (Assessment)_4020_accuracy', '3bf1cf26', 'session_type_story_count_game',\n",
        "'agt_Chest Sorter (Assessment)', 'agt_current_assessment', '562cec5f', '499edb7c', 'last_assessment_accuracy_group', 'ca11f653', '2040_proc', 4100, 4020,\n",
        "'0db6d71d', '37ee8496', 'acc_Bird Measurer (Assessment)', 'Game_CRYSTALCAVES', 'acc_game_session_correct_true', 'lgt_Bird Measurer (Assessment)',\n",
        "'ata_Chest Sorter (Assessment)', '907a054b', '3babcb9b', 'acc_Mushroom Sorter (Assessment)', 'acc_Chest Sorter (Assessment)', 'Bottle Filler (Activity)_4020',\n",
        "'Game_TREETOPCITY', 'session_type_story_count_activity', 'last_activity_world_is_the_same', 'last_assessment_world_is_the_same', 'last_assessment_title_is_the_same',\n",
        "]\n",
        "\n",
        "categorical_features =  [f for f in model_feats if f in ['session_title',\n",
        "                                                         'last_game_session_title',\n",
        "                                                         'last_assessment_session_title',\n",
        "                                                         'last_activity_session_title',                                                         \n",
        "                                                         'last_assessment_accuracy_group']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shx67TCGCDk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "nmodels = NMODELS\n",
        "nfolds = NFOLDS\n",
        "num_boost_round = 1600 \n",
        "early_stopping_rounds = None \n",
        "verbose_eval = 2000\n",
        "lgb_params = {\n",
        "          'num_leaves': 19, \n",
        "          'min_data_in_leaf': 160,\n",
        "          'min_child_weight': 0.03,\n",
        "          'bagging_fraction' : 0.7,\n",
        "          'feature_fraction' : 0.8,\n",
        "          'learning_rate' : 0.01,\n",
        "          'max_depth': -1,\n",
        "          'reg_alpha': 0.02,\n",
        "          'reg_lambda': 0.12,\n",
        "          'objective': 'regression',\n",
        "          'seed': 1337,\n",
        "          'feature_fraction_seed': 1337,\n",
        "          'bagging_seed': 1337,\n",
        "          'drop_seed': 1337,\n",
        "          'data_random_seed': 1337,\n",
        "          'boosting_type': 'gbdt',\n",
        "          'verbose': 100,\n",
        "          'boost_from_average': False,\n",
        "          'metric':'rmse'\n",
        "}        \n",
        "\n",
        "y_oof_lgb, rmse_scores,  oof_cohen_scores = run_lgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )\n",
        "\n",
        "feature_importances = pd.read_csv(\"./feature_importance.csv\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=feature_importances[:50], x='average', y='feature', orient='h')\n",
        "plt.title('Feature importances (LGB)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_train[\"accuracy_group\"].values.reshape(-1, 1) - y_oof_lgb.reshape(-1, 1))\n",
        "plt.title('Distribution of errors (LGB)')\n",
        "plt.show()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jszMHFe0CDsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('RMSE LGB')\n",
        "plt.hist(rmse_scores, bins=100)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('QWK LGB')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'LGB {nmodels} rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcldjAW1CDq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = calc_bound (y_oof_lgb, df_train[\"accuracy_group\"])\n",
        "\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_lgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('LGB QWK (percentile opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV-iCJbgCTJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y_oof_lgb)\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_lgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('LGB QWK (bayesian opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElXH9PmgCYE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "nmodels = NMODELS\n",
        "nfolds = NFOLDS\n",
        "num_boost_round = 1600 \n",
        "early_stopping_rounds = None \n",
        "verbose_eval = 2000\n",
        "lgb_params = {\n",
        "          'num_leaves': 19, \n",
        "          'min_data_in_leaf': 160,\n",
        "          'min_child_weight': 0.03,\n",
        "          'bagging_fraction' : 0.7,\n",
        "          'feature_fraction' : 0.8,\n",
        "          'learning_rate' : 0.01,\n",
        "          'max_depth': -1,\n",
        "          'reg_alpha': 0.02,\n",
        "          'reg_lambda': 0.12,\n",
        "          'objective': 'binary',\n",
        "          'seed': 1337,\n",
        "          'feature_fraction_seed': 1337,\n",
        "          'bagging_seed': 1337,\n",
        "          'drop_seed': 1337,\n",
        "          'data_random_seed': 1337,\n",
        "          'boosting_type': 'gbdt',\n",
        "          'verbose': 100,\n",
        "          'boost_from_average': False,\n",
        "          'metric':'auc'\n",
        "}        \n",
        "\n",
        "categorical_features =  [f for f in model_feats if f in ['session_title',\n",
        "                                                         'last_game_session_title',\n",
        "                                                         'last_assessment_session_title',\n",
        "                                                         'last_activity_session_title',                                                         \n",
        "                                                         'last_assessment_accuracy_group']]\n",
        "    \n",
        "y_lgb_solved_oof, auc_score = run_lgb_auc_ensemble (df_train, df_train[\"accuracy_group\"].map(lambda x: 0 if x ==0 else 1) , \"solved\",  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )\n",
        "y_lgb_first_oof, auc_score = run_lgb_auc_ensemble (df_train, df_train[\"accuracy_group\"].map(lambda x: 1 if x ==3 else 0) , \"first\",  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zgOBUjoClXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmodels = NMODELS\n",
        "nfolds = NFOLDS\n",
        "num_boost_round = 600 \n",
        "early_stopping_rounds = None \n",
        "verbose_eval = 1000\n",
        "\n",
        "xgb_params = {\n",
        "            'objective':'reg:squarederror',\n",
        "            'eval_metric':'rmse',\n",
        "            'seed': 1337,\n",
        "            'colsample_bytree': 0.8,                 \n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 9,\n",
        "            'subsample': 0.7,\n",
        "            'min_child_weight':3,\n",
        "            'gamma':0.25,\n",
        "            }    \n",
        "\n",
        "categorical_features =  [f for f in model_feats if f in ['session_title',\n",
        "                                                         'last_game_session_title',\n",
        "                                                         'last_assessment_session_title',\n",
        "                                                         'last_activity_session_title',                                                         \n",
        "                                                         'last_assessment_accuracy_group']]\n",
        "\n",
        "y_oof_xgb, rmse_scores,  oof_cohen_scores = run_xgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  xgb_params, nmodels, nfolds, verbose_eval )\n",
        "\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.hist(df_train[\"accuracy_group\"].values.reshape(-1, 1) - y_oof_xgb.reshape(-1, 1))\n",
        "plt.title('Distribution of errors (XGB)')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT3PBoDACldt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('RMSE XGB')\n",
        "plt.hist(rmse_scores, bins=100)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('QWK XGB')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'XGB {nmodels} rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vM9Ay3tCrvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = calc_bound (y_oof_xgb, df_train[\"accuracy_group\"])\n",
        "\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_xgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('XGB QWK (percentile opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTU0NTHCr2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y_oof_xgb)\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_xgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('XGB QWK (bayesian opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ph4U1M1CwZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"accuracy_group\"] = df_train[\"accuracy_group\"].values\n",
        "df[\"sample\"] = df_train[\"sample\"].values\n",
        "df[\"installation_id\"] = df_train[\"installation_id\"].values\n",
        "df[\"y\"] = y_oof_lgb\n",
        "df[\"y_xgb\"] = y_oof_xgb\n",
        "df[\"solved\"] = y_lgb_solved_oof\n",
        "df[\"first\"] = y_lgb_first_oof"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpQrFxIOCwfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = (2*0.7*df[\"y\"] + 2*0.3*df[\"y_xgb\"] + 2*3*df[\"solved\"]  + 3*df[\"first\"])/5\n",
        "rmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y, df_train[\"installation_id\"].values, nsamples=5000)\n",
        "np.median(rmse_scores),  np.median(oof_cohen_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2tlMwjOCwlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = calc_bound (y, df_train[\"accuracy_group\"])\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('QWK (percentile opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n",
        "\n",
        "a,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y)\n",
        "\n",
        "oof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(' QWK (bayesian opt)')\n",
        "plt.hist(oof_cohen_scores, bins=100)\n",
        "plt.show()\n",
        "\n",
        "print (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtS55CSdC8FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df_train\n",
        "gc.collect()\n",
        "psutil.virtual_memory()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFCGqQIeC8KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "y_test_lgb = predict_lgb (NMODELS, NFOLDS) \n",
        "y_test_xgb = predict_xgb (NMODELS, NFOLDS) \n",
        "y_test_auc_solved = predict_auc_lgb ( \"solved\", NMODELS, NFOLDS)\n",
        "y_test_auc_first = predict_auc_lgb ( \"first\", NMODELS, NFOLDS)\n",
        "\n",
        "\n",
        "y_test = (2*0.7*y_test_lgb + 2*0.3*y_test_xgb + 2*3*y_test_auc_solved  + 3*y_test_auc_first)/5\n",
        "y_test = round_prediction (y_test,a,b,c)\n",
        "\n",
        "submission = pd.DataFrame()\n",
        "submission[\"installation_id\"] = test[\"installation_id\"]\n",
        "submission[\"accuracy_group\"] = y_test.astype(int)\n",
        "submission.to_csv(\"submission.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}